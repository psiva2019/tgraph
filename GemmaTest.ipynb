{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pytesseract opencv-python-headless\n",
    "#pip install llama-cpp-python\n",
    "#!brew install tesseract -- install tessarct in mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from llama_cpp import Llama\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/sivaramakrishnan/Development/Donut/sample.png\"\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "model_file = \"/Users/sivaramakrishnan/.cache/lm-studio/models/Mozilla/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/lmstudio-ai/gemma-2b-it-GGUF/gemma-2b-it-q8_0.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Payslip\\nZoonodle Inc\\n\\n21023 Pearson Point Road\\nGateway Avenue\\n\\nDate of Joining : 2018-06-23 Employee name : Sally Harley\\nPay Period : August 2021 Designation : Marketing Executive\\nWorked Days 126 Department : Marketing\\nEarnings Amount\\nBasic 10000\\nIncentive Pay 1000\\nHouse Rent Allowance 400\\nMeal Allowance 200\\nTotal Earnings 11600\\nDeductions Amount\\nProvident Fund 1200\\nProfesional Tax 500\\nLoan 400\\nTotal Deductions 2100\\nNet Pay 9500\\n9500\\n\\nNine Thousand Five Hundred\\n\\nEmployer Signature\\n\\nEmployee Signature\\n\\nThis is system generated payslip\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the path to Tesseract executable (adjust this based on your installation)\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'\n",
    "\n",
    "# Read the image (replace 'photo.jpg' with your image file path)\n",
    "img = cv2.imread(file_path)\n",
    "\n",
    "# Configuration for Tesseract (you can adjust this as needed)\n",
    "config = ('-l eng --oem 1 --psm 3')\n",
    "\n",
    "# Extract text from the image\n",
    "extracted_text = pytesseract.image_to_string(img, config=config)\n",
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 164 tensors from /Users/sivaramakrishnan/.cache/lm-studio/models/Mozilla/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/lmstudio-ai/gemma-2b-it-GGUF/gemma-2b-it-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:   37 tensors\n",
      "llama_model_loader: - type q8_0:  127 tensors\n",
      "llm_load_vocab: special tokens cache size = 388\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  2539.95 MiB, ( 2540.02 / 10922.67)\n",
      "llm_load_tensors: offloading 18 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 19/19 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   531.52 MiB\n",
      "llm_load_tensors:      Metal buffer size =  2539.94 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   504.25 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     5.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 601\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'general.file_type': '7', 'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'general.architecture': 'gemma', 'gemma.attention.head_count_kv': '1', 'gemma.feed_forward_length': '16384', 'tokenizer.ggml.bos_token_id': '2', 'gemma.embedding_length': '2048', 'gemma.block_count': '18', 'tokenizer.ggml.unknown_token_id': '3', 'gemma.attention.key_length': '256', 'gemma.context_length': '8192', 'general.name': 'gemma-2b-it', 'gemma.attention.value_length': '256', 'gemma.attention.head_count': '8'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=model_file,\n",
    "            n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1224.03 ms\n",
      "llama_print_timings:      sample time =     456.04 ms /   236 runs   (    1.93 ms per token,   517.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1223.39 ms /   193 tokens (    6.34 ms per token,   157.76 tokens per second)\n",
      "llama_print_timings:        eval time =   16025.60 ms /   235 runs   (   68.19 ms per token,    14.66 tokens per second)\n",
      "llama_print_timings:       total time =   18276.59 ms /   428 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"document_type\": \"Payslip\",\\n  \"company_name\": \"Zoonodle Inc\",\\n  \"address\": \"21023 Pearson Point Road, Gateway Avenue\",\\n  \"date_of_joining\": \"2018-06-23\",\\n  \"employee_name\": \"Sally Harley\",\\n  \"pay_period\": \"August 2021\",\\n  \"designation\": \"Marketing Executive\",\\n  \"worked_days\": 126,\\n  \"earnings\": {\\n    \"basic\": 10000,\\n    \"incentive_pay\": 1000,\\n    \"house_rent_allowance\": 400,\\n    \"meal_allowance\": 200\\n  },\\n  \"deductions\": {\\n    \"provident_fund\": 1200,\\n    \"professional_tax\": 500,\\n    \"loan\": 400\\n  },\\n  \"net_pay\": 9500\\n}\\n```'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"<INST> extract data in json format with the document type </INST>\\n\" + extracted_text\n",
    "response = llm(instruction, max_tokens=5000)\n",
    "rtext = response['choices'][0]['text']\n",
    "rtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"document_type\": \"Payslip\",\\n  \"company_name\": \"Zoonodle Inc\",\\n  \"address\": \"21023 Pearson Point Road, Gateway Avenue\",\\n  \"date_of_joining\": \"2018-06-23\",\\n  \"employee_name\": \"Sally Harley\",\\n  \"pay_period\": \"August 2021\",\\n  \"designation\": \"Marketing Executive\",\\n  \"worked_days\": 126,\\n  \"earnings\": {\\n    \"basic\": 10000,\\n    \"incentive_pay\": 1000,\\n    \"house_rent_allowance\": 400,\\n    \"meal_allowance\": 200\\n  },\\n  \"deductions\": {\\n    \"provident_fund\": 1200,\\n    \"professional_tax\": 500,\\n    \"loan\": 400\\n  },\\n  \"net_pay\": 9500\\n}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_text = rtext[rtext.find(\"{\"):rtext.rfind(\"}\")+1]\n",
    "json_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_type': 'Payslip',\n",
       " 'company_name': 'Zoonodle Inc',\n",
       " 'address': '21023 Pearson Point Road, Gateway Avenue',\n",
       " 'date_of_joining': '2018-06-23',\n",
       " 'employee_name': 'Sally Harley',\n",
       " 'pay_period': 'August 2021',\n",
       " 'designation': 'Marketing Executive',\n",
       " 'worked_days': 126,\n",
       " 'earnings': {'basic': 10000,\n",
       "  'incentive_pay': 1000,\n",
       "  'house_rent_allowance': 400,\n",
       "  'meal_allowance': 200},\n",
       " 'deductions': {'provident_fund': 1200, 'professional_tax': 500, 'loan': 400},\n",
       " 'net_pay': 9500}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(json_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
